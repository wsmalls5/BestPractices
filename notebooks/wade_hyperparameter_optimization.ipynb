{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demanding-portugal",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization - WS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-venture",
   "metadata": {},
   "source": [
    "Each machine learning model has a set of parameters that are initialized by the human prior to the learning process that have an effect on the learning algorithm of the model. These are called Hyperparameters, and can be tuned and optimized to further improve a models performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-exclusion",
   "metadata": {},
   "source": [
    "## Load in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-emperor",
   "metadata": {},
   "source": [
    "Let's first begin by importing the featurized, split, scaled, normalized data sets from notebook 3-modeling classic models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "restricted-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "\n",
    "# Set a random seed to ensure reproducibility across runs\n",
    "RNG_SEED = 42\n",
    "np.random.seed(RNG_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thrown-ukraine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train DataFrame shape: (2000, 177)\n",
      "df_val DataFrame shape: (200, 177)\n",
      "df_test DataFrame shape: (200, 177)\n",
      "\n",
      "df_train_target DataFrame shape: (2000, 1)\n",
      "df_val_target DataFrame shape: (200, 1)\n",
      "df_test_target DataFrame shape: (200, 1)\n"
     ]
    }
   ],
   "source": [
    "PATH = os.getcwd()\n",
    "\n",
    "# X df\n",
    "train_path = os.path.join(PATH, '../data/cp_train_processed.csv')\n",
    "val_path = os.path.join(PATH, '../data/cp_val_processed.csv')\n",
    "test_path = os.path.join(PATH, '../data/cp_test_processed.csv')\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_val = pd.read_csv(val_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "# y df\n",
    "train_path_y = os.path.join(PATH, '../data/cp_train_target.csv')\n",
    "val_path_y = os.path.join(PATH, '../data/cp_val_target.csv')\n",
    "test_path_y = os.path.join(PATH, '../data/cp_test_target.csv')\n",
    "\n",
    "df_train_target = pd.read_csv(train_path_y)\n",
    "df_val_target = pd.read_csv(val_path_y)\n",
    "df_test_target = pd.read_csv(test_path_y)\n",
    "\n",
    "\n",
    "print(f'df_train DataFrame shape: {df_train.shape}')\n",
    "print(f'df_val DataFrame shape: {df_val.shape}')\n",
    "print(f'df_test DataFrame shape: {df_test.shape}''\\n')\n",
    "\n",
    "print(f'df_train_target DataFrame shape: {df_train_target.shape}')\n",
    "print(f'df_val_target DataFrame shape: {df_val_target.shape}')\n",
    "print(f'df_test_target DataFrame shape: {df_test_target.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-switch",
   "metadata": {},
   "source": [
    "Let's quickly see what the train data frame looks like, just for fun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polar-porcelain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.049496</td>\n",
       "      <td>-0.049293</td>\n",
       "      <td>-0.038091</td>\n",
       "      <td>-0.033664</td>\n",
       "      <td>-0.036180</td>\n",
       "      <td>-0.033839</td>\n",
       "      <td>-0.018897</td>\n",
       "      <td>-0.013881</td>\n",
       "      <td>-0.045918</td>\n",
       "      <td>-0.049305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169828</td>\n",
       "      <td>0.087697</td>\n",
       "      <td>-0.030553</td>\n",
       "      <td>-0.020227</td>\n",
       "      <td>0.325807</td>\n",
       "      <td>-0.016071</td>\n",
       "      <td>-0.012299</td>\n",
       "      <td>0.145706</td>\n",
       "      <td>0.136940</td>\n",
       "      <td>-0.055059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.046658</td>\n",
       "      <td>-0.047568</td>\n",
       "      <td>-0.022087</td>\n",
       "      <td>-0.017007</td>\n",
       "      <td>0.083449</td>\n",
       "      <td>-0.101719</td>\n",
       "      <td>0.111022</td>\n",
       "      <td>-0.011695</td>\n",
       "      <td>0.081329</td>\n",
       "      <td>-0.041540</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049002</td>\n",
       "      <td>-0.057925</td>\n",
       "      <td>-0.028256</td>\n",
       "      <td>0.212334</td>\n",
       "      <td>-0.024335</td>\n",
       "      <td>-0.047894</td>\n",
       "      <td>-0.034548</td>\n",
       "      <td>0.013040</td>\n",
       "      <td>0.053715</td>\n",
       "      <td>-0.057497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.032543</td>\n",
       "      <td>-0.032056</td>\n",
       "      <td>0.037335</td>\n",
       "      <td>0.068315</td>\n",
       "      <td>0.073449</td>\n",
       "      <td>0.157559</td>\n",
       "      <td>0.063540</td>\n",
       "      <td>-0.015627</td>\n",
       "      <td>0.065435</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026901</td>\n",
       "      <td>-0.015855</td>\n",
       "      <td>-0.035402</td>\n",
       "      <td>-0.013003</td>\n",
       "      <td>-0.031106</td>\n",
       "      <td>-0.036161</td>\n",
       "      <td>0.024058</td>\n",
       "      <td>-0.020971</td>\n",
       "      <td>-0.024492</td>\n",
       "      <td>-0.093283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.064185</td>\n",
       "      <td>-0.060748</td>\n",
       "      <td>-0.070468</td>\n",
       "      <td>-0.057946</td>\n",
       "      <td>-0.062284</td>\n",
       "      <td>-0.032347</td>\n",
       "      <td>-0.054070</td>\n",
       "      <td>-0.013269</td>\n",
       "      <td>-0.067776</td>\n",
       "      <td>-0.073402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053875</td>\n",
       "      <td>-0.056931</td>\n",
       "      <td>-0.031278</td>\n",
       "      <td>-0.019205</td>\n",
       "      <td>-0.027656</td>\n",
       "      <td>-0.050643</td>\n",
       "      <td>0.004904</td>\n",
       "      <td>-0.046145</td>\n",
       "      <td>-0.047536</td>\n",
       "      <td>0.142243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016841</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>-0.043320</td>\n",
       "      <td>-0.048758</td>\n",
       "      <td>-0.059175</td>\n",
       "      <td>-0.023704</td>\n",
       "      <td>-0.066010</td>\n",
       "      <td>-0.009723</td>\n",
       "      <td>-0.053324</td>\n",
       "      <td>-0.053789</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041007</td>\n",
       "      <td>-0.048359</td>\n",
       "      <td>-0.023493</td>\n",
       "      <td>-0.014270</td>\n",
       "      <td>-0.020410</td>\n",
       "      <td>-0.039843</td>\n",
       "      <td>-0.028725</td>\n",
       "      <td>-0.034276</td>\n",
       "      <td>-0.035182</td>\n",
       "      <td>0.013361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.262353</td>\n",
       "      <td>0.267779</td>\n",
       "      <td>0.156791</td>\n",
       "      <td>0.058086</td>\n",
       "      <td>0.062452</td>\n",
       "      <td>-0.032392</td>\n",
       "      <td>0.054026</td>\n",
       "      <td>-0.013287</td>\n",
       "      <td>0.050997</td>\n",
       "      <td>0.058035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006468</td>\n",
       "      <td>0.072964</td>\n",
       "      <td>-0.030027</td>\n",
       "      <td>-0.019496</td>\n",
       "      <td>-0.024961</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>-0.038153</td>\n",
       "      <td>0.129422</td>\n",
       "      <td>0.120138</td>\n",
       "      <td>-0.079493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-0.051198</td>\n",
       "      <td>-0.053064</td>\n",
       "      <td>-0.033001</td>\n",
       "      <td>-0.076308</td>\n",
       "      <td>-0.075941</td>\n",
       "      <td>0.066787</td>\n",
       "      <td>-0.071204</td>\n",
       "      <td>-0.017473</td>\n",
       "      <td>-0.033392</td>\n",
       "      <td>-0.096661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057940</td>\n",
       "      <td>-0.070259</td>\n",
       "      <td>-0.041836</td>\n",
       "      <td>-0.025442</td>\n",
       "      <td>0.044250</td>\n",
       "      <td>-0.034717</td>\n",
       "      <td>-0.051432</td>\n",
       "      <td>0.023407</td>\n",
       "      <td>0.018281</td>\n",
       "      <td>-0.104535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>-0.050158</td>\n",
       "      <td>-0.048081</td>\n",
       "      <td>-0.073605</td>\n",
       "      <td>-0.048087</td>\n",
       "      <td>-0.066054</td>\n",
       "      <td>-0.040275</td>\n",
       "      <td>-0.067324</td>\n",
       "      <td>-0.016521</td>\n",
       "      <td>-0.059534</td>\n",
       "      <td>-0.025973</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026125</td>\n",
       "      <td>-0.007800</td>\n",
       "      <td>0.043091</td>\n",
       "      <td>-0.023667</td>\n",
       "      <td>-0.027401</td>\n",
       "      <td>-0.021094</td>\n",
       "      <td>0.395452</td>\n",
       "      <td>-0.051350</td>\n",
       "      <td>-0.053786</td>\n",
       "      <td>-0.087589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>-0.026411</td>\n",
       "      <td>-0.028134</td>\n",
       "      <td>-0.024171</td>\n",
       "      <td>-0.068100</td>\n",
       "      <td>-0.060983</td>\n",
       "      <td>0.089427</td>\n",
       "      <td>-0.095342</td>\n",
       "      <td>-0.023396</td>\n",
       "      <td>-0.066082</td>\n",
       "      <td>0.055865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056896</td>\n",
       "      <td>-0.001830</td>\n",
       "      <td>0.019166</td>\n",
       "      <td>-0.033358</td>\n",
       "      <td>-0.031977</td>\n",
       "      <td>-0.008546</td>\n",
       "      <td>-0.034781</td>\n",
       "      <td>-0.075362</td>\n",
       "      <td>-0.077117</td>\n",
       "      <td>0.032149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-0.028890</td>\n",
       "      <td>-0.027410</td>\n",
       "      <td>-0.053342</td>\n",
       "      <td>-0.041073</td>\n",
       "      <td>-0.044136</td>\n",
       "      <td>-0.068853</td>\n",
       "      <td>-0.038450</td>\n",
       "      <td>-0.028243</td>\n",
       "      <td>-0.050940</td>\n",
       "      <td>0.011519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069320</td>\n",
       "      <td>0.209096</td>\n",
       "      <td>0.006663</td>\n",
       "      <td>-0.040016</td>\n",
       "      <td>-0.038256</td>\n",
       "      <td>0.186852</td>\n",
       "      <td>-0.081009</td>\n",
       "      <td>0.093118</td>\n",
       "      <td>0.076140</td>\n",
       "      <td>0.170794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.049496 -0.049293 -0.038091 -0.033664 -0.036180 -0.033839 -0.018897   \n",
       "1    -0.046658 -0.047568 -0.022087 -0.017007  0.083449 -0.101719  0.111022   \n",
       "2    -0.032543 -0.032056  0.037335  0.068315  0.073449  0.157559  0.063540   \n",
       "3    -0.064185 -0.060748 -0.070468 -0.057946 -0.062284 -0.032347 -0.054070   \n",
       "4     0.016841  0.026828 -0.043320 -0.048758 -0.059175 -0.023704 -0.066010   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1995  0.262353  0.267779  0.156791  0.058086  0.062452 -0.032392  0.054026   \n",
       "1996 -0.051198 -0.053064 -0.033001 -0.076308 -0.075941  0.066787 -0.071204   \n",
       "1997 -0.050158 -0.048081 -0.073605 -0.048087 -0.066054 -0.040275 -0.067324   \n",
       "1998 -0.026411 -0.028134 -0.024171 -0.068100 -0.060983  0.089427 -0.095342   \n",
       "1999 -0.028890 -0.027410 -0.053342 -0.041073 -0.044136 -0.068853 -0.038450   \n",
       "\n",
       "             7         8         9  ...       167       168       169  \\\n",
       "0    -0.013881 -0.045918 -0.049305  ...  0.169828  0.087697 -0.030553   \n",
       "1    -0.011695  0.081329 -0.041540  ... -0.049002 -0.057925 -0.028256   \n",
       "2    -0.015627  0.065435  0.006373  ... -0.026901 -0.015855 -0.035402   \n",
       "3    -0.013269 -0.067776 -0.073402  ... -0.053875 -0.056931 -0.031278   \n",
       "4    -0.009723 -0.053324 -0.053789  ... -0.041007 -0.048359 -0.023493   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1995 -0.013287  0.050997  0.058035  ...  0.006468  0.072964 -0.030027   \n",
       "1996 -0.017473 -0.033392 -0.096661  ... -0.057940 -0.070259 -0.041836   \n",
       "1997 -0.016521 -0.059534 -0.025973  ... -0.026125 -0.007800  0.043091   \n",
       "1998 -0.023396 -0.066082  0.055865  ...  0.056896 -0.001830  0.019166   \n",
       "1999 -0.028243 -0.050940  0.011519  ...  0.069320  0.209096  0.006663   \n",
       "\n",
       "           170       171       172       173       174       175       176  \n",
       "0    -0.020227  0.325807 -0.016071 -0.012299  0.145706  0.136940 -0.055059  \n",
       "1     0.212334 -0.024335 -0.047894 -0.034548  0.013040  0.053715 -0.057497  \n",
       "2    -0.013003 -0.031106 -0.036161  0.024058 -0.020971 -0.024492 -0.093283  \n",
       "3    -0.019205 -0.027656 -0.050643  0.004904 -0.046145 -0.047536  0.142243  \n",
       "4    -0.014270 -0.020410 -0.039843 -0.028725 -0.034276 -0.035182  0.013361  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1995 -0.019496 -0.024961  0.079500 -0.038153  0.129422  0.120138 -0.079493  \n",
       "1996 -0.025442  0.044250 -0.034717 -0.051432  0.023407  0.018281 -0.104535  \n",
       "1997 -0.023667 -0.027401 -0.021094  0.395452 -0.051350 -0.053786 -0.087589  \n",
       "1998 -0.033358 -0.031977 -0.008546 -0.034781 -0.075362 -0.077117  0.032149  \n",
       "1999 -0.040016 -0.038256  0.186852 -0.081009  0.093118  0.076140  0.170794  \n",
       "\n",
       "[2000 rows x 177 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "appreciated-wages",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>135.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>127.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>67.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>46.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>101.504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>74.475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target\n",
       "0      66.392\n",
       "1     109.956\n",
       "2     135.520\n",
       "3      71.128\n",
       "4      37.183\n",
       "...       ...\n",
       "1995  127.800\n",
       "1996   67.864\n",
       "1997   46.806\n",
       "1998  101.504\n",
       "1999   74.475\n",
       "\n",
       "[2000 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "variable-insured",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 66.392, 109.956, 135.52 , ...,  46.806, 101.504,  74.475])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll need the target values in a 1d array for the model fit function\n",
    "df_train_target = df_train_target.values.ravel()\n",
    "df_train_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-contemporary",
   "metadata": {},
   "source": [
    "Now let's set up an empty data frame that we will use to store model results, and a dictionary of the model names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "analyzed-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classics = pd.DataFrame(columns=['model_name',\n",
    "                                    'model_name_pretty',\n",
    "                                    'model_params',\n",
    "                                    'fit_time',\n",
    "                                    'r2_train',\n",
    "                                    'mae_train',\n",
    "                                    'rmse_train',\n",
    "                                    'r2_val',\n",
    "                                    'mae_val',\n",
    "                                    'rmse_val'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-willow",
   "metadata": {},
   "source": [
    "Also, to be able to feed our data into the learning models, let's rename them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "manufactured-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train\n",
    "y_train = df_train_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-restriction",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-bridge",
   "metadata": {},
   "source": [
    "In this notebook, I will use both the Grid Search and Random Search optimization techniques for various models. We can then compare the models performance following hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-brazilian",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Grid Search is method whereby all combination subsets of hyperparameters are stepped through sequnetially and exhuastively. the optimal subset is then identified. This method is computationally heavy and thus can be very time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-interpretation",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "\n",
    "Random search does essentialy the same thing as the Grid Search method, but saves computational power by randomly assigning parameter configurations and testing only (n) # of samples that we can define. The downsides to this method is you may not discover the absolute best combination of parameters for your model, but at a lower cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rocky-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-funeral",
   "metadata": {},
   "source": [
    "## 1 - Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-coordinate",
   "metadata": {},
   "source": [
    "The first thing to do for a specfic model is identify the names and preset values for all the parameters for a given model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-destiny",
   "metadata": {},
   "source": [
    "### Get Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "indian-modeling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'criterion': 'mse',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create the base model, then display hyperparameters\n",
    "rfr = RandomForestRegressor(random_state = 42)\n",
    "pprint(rfr.get_params())\n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-shirt",
   "metadata": {},
   "source": [
    "SciKit-Learn docs tells us that the most important hyperparameters of the 18 available are:\n",
    "- n_estimators : the # of trees in the forest\n",
    "- max_features : the # of features to consider for splitting at each node\n",
    "- max_depth : max # of levels in each decision tree\n",
    "- min_samples_split : min # of data points placed in node before node is split\n",
    "- min_samples_leaf : min # of data points allowed in a lead node\n",
    "- bootstrap : method for sampling datapoints\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-audience",
   "metadata": {},
   "source": [
    "### Build the Grid\n",
    "\n",
    "Next we'll create an array of values to vary each hyperparameter across. In the case of a `Grid_Search` each combination of hyperparameter values will be applied and tested. Altogether that amounts to some 4,320 unique sets of hyperparameters based on the arrays below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "incomplete-plate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "pprint(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-milton",
   "metadata": {},
   "source": [
    "Let's start with a `random_search`, then later compare it to a `grid_search` to see how the model improves vs the time it takes to compute. We may find that a random search saves time and finds nearly the best set of hyperparameters - let's find out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-account",
   "metadata": {},
   "source": [
    "### Random Search - Function Inputs\n",
    "\n",
    "Let's tune using the following inputs: \n",
    "- `n_iter` = 10 | # of iterations or random sets of hyperparameter configurations\n",
    "- `cv` = 3 | set a k-fold cross validation with k splits of the training data - higher k reduces overfitting\n",
    "- `verbose` = 2 | best I can find is this value can be set between 0 - 3 and determines how many output log messages are displayed\n",
    "- `n_jobs` = -1 | # of concurrent processing jobs - at n = -1 all cores will be utlized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "roman-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_random = RandomizedSearchCV(estimator = rfr, param_distributions = param_grid, n_iter = 10, cv = 3, verbose = 2, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-melbourne",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "minor-biology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 24.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=RandomForestRegressor(bootstrap=True,\n",
       "                                                   ccp_alpha=0.0,\n",
       "                                                   criterion='mse',\n",
       "                                                   max_depth=None,\n",
       "                                                   max_features='auto',\n",
       "                                                   max_leaf_nodes=None,\n",
       "                                                   max_samples=None,\n",
       "                                                   min_impurity_decrease=0.0,\n",
       "                                                   min_impurity_split=None,\n",
       "                                                   min_samples_leaf=1,\n",
       "                                                   min_samples_split=2,\n",
       "                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                   n_estimators=100,\n",
       "                                                   n_jobs=None, oob_score=Fals...\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-disney",
   "metadata": {},
   "source": [
    "### Retrieve Best Parameters from Search & Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-flood",
   "metadata": {},
   "source": [
    "A function in `sklearn` allows us to view the best parameters from the random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "large-yield",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 2000,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 50,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-tractor",
   "metadata": {},
   "source": [
    "Now we can use an evaluation function to compare the model using the preset hyperparameters vs the Random Search optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "wrong-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y_act):\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    r2 = r2_score(y_act, y_pred)\n",
    "    mae = mean_absolute_error(y_act, y_pred)\n",
    "    rmse_val = mean_squared_error(y_act, y_pred, squared=False)\n",
    "    errors = abs(y_pred - y_act)\n",
    "    mape = 100 * np.mean(errors / y_act)\n",
    "    \n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%'.format(accuracy))\n",
    "    print('r2: {:0.3f}'.format(r2))\n",
    "    print('\\n')\n",
    "    \n",
    "    return accuracy, r2, mae, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "intellectual-knife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 1.5114 degrees\n",
      "Accuracy = 98.52%\n",
      "r2: 0.997\n",
      "\n",
      "\n",
      "Model Performance\n",
      "Average Error: 1.5662 degrees\n",
      "Accuracy = 98.48%\n",
      "r2: 0.997\n",
      "\n",
      "\n",
      "Improvement of -0.04%.\n"
     ]
    }
   ],
   "source": [
    "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_train, y_train)\n",
    "\n",
    "best_random = rfr_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_train, y_train)\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy[0] - base_accuracy[0]) / base_accuracy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-aluminum",
   "metadata": {},
   "source": [
    "In this case, the preset hyperparameters lend to a better learning model than the best of the 30 iterations tried."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-electron",
   "metadata": {},
   "source": [
    "### Repeat w/ More Iterations\n",
    "\n",
    "Let's double the number of iterations and see if we can find a configuration of hyperparameters that out performs the preset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wireless-fault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 19.0min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 34.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=RandomForestRegressor(bootstrap=True,\n",
       "                                                   ccp_alpha=0.0,\n",
       "                                                   criterion='mse',\n",
       "                                                   max_depth=None,\n",
       "                                                   max_features='auto',\n",
       "                                                   max_leaf_nodes=None,\n",
       "                                                   max_samples=None,\n",
       "                                                   min_impurity_decrease=0.0,\n",
       "                                                   min_impurity_split=None,\n",
       "                                                   min_samples_leaf=1,\n",
       "                                                   min_samples_split=2,\n",
       "                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                   n_estimators=100,\n",
       "                                                   n_jobs=None, oob_score=Fals...\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr_random = RandomizedSearchCV(estimator = rfr, param_distributions = param_grid, n_iter = 20, cv = 3, verbose = 2, n_jobs = -1)\n",
    "\n",
    "rfr_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "romance-polls",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 600,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 70,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "searching-savings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 1.5114 degrees\n",
      "Accuracy = 98.52%\n",
      "r2: 0.997\n",
      "\n",
      "\n",
      "Model Performance\n",
      "Average Error: 1.1706 degrees\n",
      "Accuracy = 98.86%\n",
      "r2: 0.998\n",
      "\n",
      "\n",
      "Improvement of 0.35%.\n"
     ]
    }
   ],
   "source": [
    "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = evaluate(base_model, X_train, y_train)\n",
    "\n",
    "best_random = rfr_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_train, y_train)\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy[0] - base_accuracy[0]) / base_accuracy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-brass",
   "metadata": {},
   "source": [
    "### Test models against validation data split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-investigation",
   "metadata": {},
   "source": [
    "To see the effects of the tuning process, we will take the two models, one with the preset and one with the optimized hyperparameters, and test each one against the validation data set. We should expect to see a similar model performance improvement as what we see above via the `evaluate` function (the difference will be, the `evaluate` function only compared models based on training data split, not including validation data split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-metro",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
